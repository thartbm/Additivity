---
title: "Additivity of Implicit and Explicit Motor Adaptation"
output:
  html_document:
    df_print: paged
bibliography: bibliography.bib
csl: plos.csl
link-citations: yes
---
 
In this document we pre-process and analyze data and make figures for a project where we tested the presumed additivity of implicit and explicit motor adaptation. First we did an experiment with 3 groups of participants that performed motor adaptation in conditions with the same rotation to adapt to, but designed to evoke different levels of explicit adaptation. Then we collected data from a set of other studies, and tested additivity in that data as well.

In our experiment, the first two groups got either no additional information (control group) or they got an instruction that explained the perturbation and how to counter it, and these participants would only do the rotated phase of the task once they demonstrated they understood how to counter the rotation (instructed group). The third group did not get an instruction, but before every reach training trials they were asked to orient an arrow in the direction they were planning to move their hand in order to get the cursor to the target. The idea was that asking participants to think really hard on where they would move their hand relative to the target, might cue them in on the fact that aiming your hand somewhere other than the target might be the way to optimally do the task. In effect, this would act like instructions such as used in other studies [@Werner2015; @Modchalingam2019], and in the instructed group here.

We test two forms of additivity, first the more common "strict" additivity that assumes total adaptation equals the sum of implicit and explicit adaptation:

$A = I + E$, or as it is usually used:

$I = A - E$, which leads to a relationship between implicit and explicit that should be linear and have a slope of -1.

We also test a less strict form that assumes that within each group, total adaptation can be approximated as a weighted sum of implicit and explicit adaptation.

$\hat{A} = \beta_i \cdot I + \beta_e \cdot E$, and:

$\hat{A} = A$, such that the predicted and actual adaptation should have a linear relationship with a slope of 1. If the confidence intervals for the slope parameters include -1 (strict) or 1 (loose) and exclude 0, this means that the additivity assumption is confirmed.

We install a support package, also used in other projects from our lab:

```{r}
if (!require(Reach)) {
  if (!require(devtools)) {
    install.packages(devtools)
  }
  devtools::install_github('thartbm/Reach', ref='main', force=TRUE)
}
library(Reach)
```

All the raw data for this project is available from OSF. We can use a `Reach` function to download it:

```{r}
datalist <- list()
datalist[['\\']] <- c('aiming.zip', 'control.zip', 'instructed.zip', 'demographics.csv', 'extra.zip')
Reach::downloadOSFdata(repository='kr5eh',
                       filelist=datalist,
                       folder='data',
                       overwrite=TRUE,
                       unzip=TRUE,
                       removezips=TRUE)
```

There should now be 4 subfolders in the 'data' directory: one for each of the 3 groups in our experiment, with full data sets, and a folder for the external data with preprocessed data. There should also be a file called 'demographics.csv' with information about the participants in our experiment.

This RStudio Project uses `renv` in an attempt to make sure the same (versions of) packages are used here as we used when we originally wrote the scripts (a few years ago, really). This probably took some time when starting this up. It also means that all those packages are installed in the project folder, so after being done with the project, you might want to free up some disk space by removing it.

Then we access all our custom functions:

```{r}
# this is for (pre)processing the raw data, and importing the external data:
source('R/process.R')

# this function provides access to demographics about the participants:
source('R/participants.R')

# this has functions to make figures:
source('R/figures.R')

# this file has functions that reproduce the statistical analyses:
source('R/statistics.R')
```

# Process data

This function takes the raw data files from every participant (24 per group), and converts it to a single file with 1 row per trial, and all behavior expressed in terms of angular deviations from the target. It stores one file per group. This may take a while...

```{r}
processData()
```

From this we extract the interesting data: reach deviations and aiming responses during training and the three kinds of no-cursor reach deviations. This should be much faster:

```{r}
saveTrainingdata()
saveNoCursorData()
```

Apart from the folders with raw data, there should now be a lot of other csv files in the `data` directory. We'll use those for figures as well as statistics.

# Adaptation

Let's have a look at the overall learning (or 'adaptation') and the aiming responses over time:

```{r fig.width=8, fig.height=6}
fig3_Learning_PointEstimates()
```


# Additivity

We will now do statistics on the values depicted in panel B.

The level of adaptation is estimated for every participant as the reach deviations in the last three block of 8 trials before each set of no-cursor blocks in the rotated part, minus the reach deviations in the last three blocks of 8 trials before each set of no-cursor blocks in the aligned part. We can then look for differences in the level of adaptation between the groups using a one-factor ANOVA:


```{r}
adaptationFtest()
```

That is, there is no effect of group on adaptation (F(2,69)=0.20, p=0.82, ges=.006), which means that we can't reject the null-hypothesis that adaptation is the same in each of the groups.

As a measure of implicit adaptation we take the reach deviations in the "exclude strategy" no-cursor reaches from our Process Dissociation Procedure minus the reach deviations in the no-cursor trials from aligned part. We then test if implicit adaptation is different between the groups:

```{r}
implicitFtest()
```

We see there is no effect of group on implicit no-cursor reach deviations (F(2,69)=0.59, p=.56, ges=.017), so that we can not reject the null-hypothesis that there is no difference between the groups in implicit adaptation.

For now, we estimate explicit adaptation for every participant as the difference between reach deviations in the include and exclude blocks from the Process Dissociation Procedure. Then we can test if explicit adaptation is different between the three groups:

```{r}
knitr::kable(afex::nice(explicitFtest()))
```

There is an effect of group on explicit adaptation (F(2,69)=17.56, p<.001, ges=.337), which means that explicit adaptation is different between at least two of the groups. We will do a post-hoc comparison to investigate this:

```{r}
knitr::kable(explicitPostHoc())
```

We find that we can't reject the null hypothesis that explicit adaptation is the same in the aiming and the control group (t-ratio(df=69)=1.53, p=0.28). There are differences between the instructed group and both the aiming (t-ratio(69)=-4.19, p<.001) and the control group (t-ratio(69)=-5.72, p<.001).

Of course, this difference measure between include and exclude strategy no-cursor reach deviations suffers from the same weakness that we are investigating here: it assumes that explicit and implicit adaptation are literally added by the brain AND that this is measured adequately in the include strategy conditions AND that the exclude strategy condition gives us a good measure of implicit adaptation. All of these (largely untested) assumptions need to be true for this difference measure to make sense. So we'll test that.

# Explicit measure

Apart from the difference between include and exclude strategy no-cursor reaches as a measure of explicit adaptation, we also have re-aiming responses - of course only for the aiming group. Re-aiming responses seem like a valid measure of explicit adaptation, and are certainly used more than the difference between two kinds of Process Dissociation Procedure no-cursor reach deviations. So we can test how well the two match each other. This is depicted in panel C of the above figure.

We test how well aiming responses, as a direct measure of explicit adaptation, predict the include-exclude difference, an indirect measure of explicit adaptation with a linear regression:

```{r}
explicitRegression()
```

The intercept (2.2 degrees) does not contribute (t=1.59, p=0.13), but the re-aiming response does (t=4.69, p<.001) with a slope of 0.945. The R-squared is moderate though.

This means that at least in this task and this sample, there is fairly good agreement between the difference measure and the re-aiming responses. While this will need to be confirmed in different tasks and conditions, for now we interpret this to mean that the difference measure is also a reasonable estimate for explicit adaptation in the control and instructed group.

This is NOT in line with Maresch, Werner and Donchin (EJN, 2020), and indeed a weak part of the study.


>>> well then...

# Two-rate model

The fast and slow processed in the state-spave model of adaptation proposed by Smith et al. (2006) have been said to map onto explicit and implicit adaptation. This model also has a strong additivity assumption, but predicts a specific time-course for both the slow and fast process, and so would be different from the above tests that only look at one specific point in time when everything has saturated.

```{r fig.width=8, fig.height=3}
fig4_Awareness_TwoRate()
```


We want to only use the direct measure of explicit adaptation here, so we are restricted to the aiming group. There is variability in both measures of explicit adaptation, but it does look like it could be a bi-modal distribution. Since the fast process in the two-rate model might correspond to explicit adaptation, this means there could be different model fits and different processes, if there is a bi-modal distribution of explicit adaptation in the aiming group. We test this in the include-exclude difference scores, since we want to use the aiming responses later on.

Fitting a single distribution (two parameter model) is worse than fitting a bi-modal distribution (four parameter model, AIC-based log-likelihood < .001), so we split the aiming participants in two groups. The centres of the two Gaussians used in this model (dashed black line in panel A) to are close to the means in the instructed group and the control group, so we will call them "aware aimers" (N=9) and "unaware aimers" (N=15), and split them by assigning them to the two groups by how high the relative likelihood is that they fall in one group or the other given the include-exclude difference score.

A second issue here is that the two-rate model might need a different perturbation schedule, so we tested if parameters can be recovered reasonably well from simulated data, based on the control group's reach deviations and SD (1k bootstraps). The 95% confidence interval of the means includes the original parameter value for all 4, which shows that (at least on the group level) parameters can be recovered reasonably well.

We fit the two-rate model on the average reach deviations in each of the two sub-groups, and then compare their fast process with the 95% confidence interval of the aiming responses; if they both correspond to explicit adaptation, they should be the same. We also compare their slow process with the confidence interval of the exclude-strategy reach aftereffects; if these both correspond to implicit adaptation, they should also be the same.

They are not the same however, showing that a model with a strong additivity assumption is not suitable to predict explicit and implicit adaptation.

(We can also test if the slow process predicts implicit adaptation in the control group (it doesn't). In the instructed group however, the instructions in between the aligned and rotated part of the task cause a discontinuity in responses that can't be handles by the standard two-rate model.)

# External data

So far we have only looked at 1 single experiment, and in that data set, additivity does not seem to hold. But is this a general principle that would apply to most, if not all, adaptation research? So we set out to collect a few other data sets from a variety of labs, with different equipment, tasks, research questions but in all cases there were measures of implicit adaptation (strategy exlcusion open-loop reaches) and some measure of explicit adaptation (either aiming, or PDP difference scores).

We try the 'strict additivity' and 'loose additivity' from above, and plot the slope plus confidence interval for each of the 35 subgroups.

```{r fig.height=8, fig.width=4}
  fig5_ExtraData()
```
Strict additivity can not be confirmed in any subgroup, according to our criterion, and loose additivity only in 6/35 cases (purple circles+lines on the right side). This means that additivity usually does not hold in a hopefully representative sample of data from the field.

# Alternative explanations

We also test a few other possibilities on this data. First, we plot all the normalized implicit over explicit adaptation, and then two other possible mechanisms.

```{r fig.width=8, fig.height=3}
fig6_Relations()
```

In panel A, the data is all over the place, in line with the previous figure. We have nevertheless added trends to panel A for rotations that have more than 100 participants (30, 45 and 60 degrees). For 30 and 45 degrees, the relationship might be more or less linear (though not additive) but the 60 degree group shows a different pattern, which we explore in panel B. We compare a linear model (organge line), with a capped fraction model, that assumes that implicit is a given fraction of what is left from total adaptation after we subtract explicit adaptation, but put a maximum on implicit adaptation as well. This alternative model works better than a linear regression (both have 2 parameters) but the spread in the data is very large, so neither is a really good model.

We also try a maximum likelihood estimate (MLE) in panel C (blue line) and compare it with simple additivity (red line). To fit the maximum likelihood model, we need reaonably reliable estimates of variance of both explicit and implicit adaptation, and this is only available in 127 participants. These two models have an identical form, but in MLE the wieghts of implicit and explicit are determined by their relative reliability, whereas in the additive case, the weights are just 1. In loose additivity, the weights are picked to best predict adptation, so since this did not explain adaptation sufficiently, it should perhaps not be surprising that the two more restricted models each don't seem to do a very good job either.

# Conclusion

We think that given our other analyses, it is unlikely that additivity holds in adaptation. Neither of our two alternatives provide a satisfactory explanation for how (measures of) explicit and implicit adaptation are combined when shaping behavior either. So we are left with a question to the field: what is the mechanism by which different adaptation or learning processes or combined when shaping behavior?

In the meantime, when we want to study implicit and/or explicit adaptation, we should use independent measures: measures that do not rely on the additivity assumption.

```{r}
for (target in c('svg', 'pdf')) {
  fig1_Additivity(target=target)
  #fig 2 = methods figure, not made in R
  fig3_Learning_PointEstimates(target=target)
  fig4_Awareness_TwoRate(target=target)
  fig5_ExtraData(target=target)
  fig6_Relations(target=target)
  
  # extra stuff:
  #figA_slowControl(target=target)
  #figB_fastInstructed(target=target)
}
```


# References
